
@article{ioannidis_why_2005,
	title = {Why {Most} {Published} {Research} {Findings} {Are} {False}},
	volume = {2},
	url = {http://dx.doi.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Published research findings are sometimes refuted by subsequent evidence, says Ioannidis, with ensuing confusion and disappointment.},
	number = {8},
	urldate = {2015-07-23},
	journal = {PLoS Med},
	author = {Ioannidis, John P. A.},
	month = aug,
	year = {2005},
	pages = {e124},
	file = {PLoS Full Text PDF:/Users/nathanmalmberg/Library/Application Support/Zotero/Profiles/2f8rc5sw.default/zotero/storage/CE6UQA2A/Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf}
}

@article{begley_six_2013,
	title = {Six red flags for suspect work},
	volume = {497},
	issn = {1476-4687},
	doi = {10.1038/497433a},
	language = {eng},
	number = {7450},
	journal = {Nature},
	author = {Begley, C. Glenn},
	month = may,
	year = {2013},
	pmid = {23698428},
	keywords = {Data Interpretation, Statistical, Indicators and Reagents, Observer Variation, Quality Control, Reproducibility of Results, Research Design, Scientific Misconduct},
	pages = {433--434}
}

@article{begley_drug_2012,
	title = {Drug development: {Raise} standards for preclinical cancer research},
	volume = {483},
	issn = {1476-4687},
	shorttitle = {Drug development},
	doi = {10.1038/483531a},
	language = {eng},
	number = {7391},
	journal = {Nature},
	author = {Begley, C. Glenn and Ellis, Lee M.},
	month = mar,
	year = {2012},
	pmid = {22460880},
	keywords = {Animals, Biomedical Research, Cell Line, Tumor, Clinical Trials as Topic, Drug Evaluation, Preclinical, Humans, Mice, Neoplasms, Reproducibility of Results, Research Design, Survival Analysis, Translational Medical Research, Treatment Failure},
	pages = {531--533}
}

@article{prinz_believe_2011,
	title = {Believe it or not: how much can we rely on published data on potential drug targets?},
	volume = {10},
	issn = {1474-1784},
	shorttitle = {},
	doi = {10.1038/nrd3439-c1},
	language = {eng},
	number = {9},
	journal = {Nature Reviews. Drug Discovery},
	author = {Prinz, Florian and Schlange, Thomas and Asadullah, Khusru},
	month = aug,
	year = {2011},
	pmid = {21892149},
	keywords = {Clinical Trials, Phase II as Topic, Drug Design, Drug Industry, Humans},
	pages = {712}
}

@article{mobley_survey_2013,
	title = {A survey on data reproducibility in cancer research provides insights into our limited ability to translate findings from the laboratory to the clinic},
	volume = {8},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0063221},
	abstract = {BACKGROUND: The pharmaceutical and biotechnology industries depend on findings from academic investigators prior to initiating programs to develop new diagnostic and therapeutic agents to benefit cancer patients. The success of these programs depends on the validity of published findings. This validity, represented by the reproducibility of published findings, has come into question recently as investigators from companies have raised the issue of poor reproducibility of published results from academic laboratories. Furthermore, retraction rates in high impact journals are climbing.
METHODS AND FINDINGS: To examine a microcosm of the academic experience with data reproducibility, we surveyed the faculty and trainees at MD Anderson Cancer Center using an anonymous computerized questionnaire; we sought to ascertain the frequency and potential causes of non-reproducible data. We found that ∼50\% of respondents had experienced at least one episode of the inability to reproduce published data; many who pursued this issue with the original authors were never able to identify the reason for the lack of reproducibility; some were even met with a less than "collegial" interaction.
CONCLUSIONS: These results suggest that the problem of data reproducibility is real. Biomedical science needs to establish processes to decrease the problem and adjudicate discrepancies in findings when they are discovered.},
	language = {eng},
	number = {5},
	journal = {PloS One},
	author = {Mobley, Aaron and Linder, Suzanne K. and Braeuer, Russell and Ellis, Lee M. and Zwelling, Leonard},
	year = {2013},
	pmid = {23691000},
	pmcid = {PMC3655010},
	keywords = {Data Collection, Humans, Neoplasms, Reproducibility of Results, Translational Medical Research},
	pages = {e63221}
}

@article{baker_1500_2016,
	title = {1,500 scientists lift the lid on reproducibility},
	volume = {533},
	issn = {1476-4687},
	doi = {10.1038/533452a},
	language = {eng},
	number = {7604},
	journal = {Nature},
	author = {Baker, Monya},
	year = {2016},
	pmid = {27225100},
	keywords = {Attitude, Data Interpretation, Statistical, Mentors, Periodicals as Topic, Publishing, Reproducibility of Results, Research, Research Design, Research Personnel, Research Support as Topic, Surveys and Questionnaires},
	pages = {452--454}
}

@article{cooper_reporting_2003,
	title = {Reporting research results: {Recommendations} for improving communication},
	volume = {41},
	issn = {0196-0644, 1097-6760},
	shorttitle = {Reporting research results},
	url = {http://www.annemergmed.com/article/S0196-0644(03)00085-4/fulltext},
	doi = {10.1067/mem.2003.135},
	language = {English},
	number = {4},
	urldate = {2017-05-10},
	journal = {Annals of Emergency Medicine},
	author = {Cooper, Richelle J. and Wears, Robert L. and Schriger, David L.},
	month = apr,
	year = {2003},
	pages = {561--564},
	file = {Full Text PDF:/Users/nathanmalmberg/Library/Application Support/Zotero/Profiles/2f8rc5sw.default/zotero/storage/X8BN6N4F/Cooper et al. - 2003 - Reporting research results Recommendations for im.pdf:application/pdf;Snapshot:/Users/nathanmalmberg/Library/Application Support/Zotero/Profiles/2f8rc5sw.default/zotero/storage/WU74GCU8/fulltext.html:text/html}
}

@article{fanelli_meta-assessment_2017,
	title = {Meta-assessment of bias in science},
	volume = {114},
	issn = {1091-6490},
	doi = {10.1073/pnas.1618569114},
	abstract = {Numerous biases are believed to affect the scientific literature, but their actual prevalence across disciplines is unknown. To gain a comprehensive picture of the potential imprint of bias in science, we probed for the most commonly postulated bias-related patterns and risk factors, in a large random sample of meta-analyses taken from all disciplines. The magnitude of these biases varied widely across fields and was overall relatively small. However, we consistently observed a significant risk of small, early, and highly cited studies to overestimate effects and of studies not published in peer-reviewed journals to underestimate them. We also found at least partial confirmation of previous evidence suggesting that US studies and early studies might report more extreme effects, although these effects were smaller and more heterogeneously distributed across meta-analyses and disciplines. Authors publishing at high rates and receiving many citations were, overall, not at greater risk of bias. However, effect sizes were likely to be overestimated by early-career researchers, those working in small or long-distance collaborations, and those responsible for scientific misconduct, supporting hypotheses that connect bias to situational factors, lack of mutual control, and individual integrity. Some of these patterns and risk factors might have modestly increased in intensity over time, particularly in the social sciences. Our findings suggest that, besides one being routinely cautious that published small, highly-cited, and earlier studies may yield inflated results, the feasibility and costs of interventions to attenuate biases in the literature might need to be discussed on a discipline-specific and topic-specific basis.},
	language = {eng},
	number = {14},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Fanelli, Daniele and Costas, Rodrigo and Ioannidis, John P. A.},
	month = apr,
	year = {2017},
	pmid = {28320937},
	pmcid = {PMC5389310},
	keywords = {bias, integrity, meta-analysis, meta-research, misconduct},
	pages = {3714--3719}
}

@article{ioannidis_reproducibility_2017,
	title = {The {Reproducibility} {Wars}: {Successful}, {Unsuccessful}, {Uninterpretable}, {Exact}, {Conceptual}, {Triangulated}, {Contested} {Replication}},
	volume = {63},
	issn = {1530-8561},
	shorttitle = {The {Reproducibility} {Wars}},
	doi = {10.1373/clinchem.2017.271965},
	language = {eng},
	number = {5},
	journal = {Clinical Chemistry},
	author = {Ioannidis, John P. A.},
	month = may,
	year = {2017},
	pmid = {28298413},
	pages = {943--945}
}

@article{ioannidis_acknowledging_2017,
	title = {Acknowledging and {Overcoming} {Nonreproducibility} in {Basic} and {Preclinical} {Research}},
	volume = {317},
	issn = {1538-3598},
	doi = {10.1001/jama.2017.0549},
	language = {eng},
	number = {10},
	journal = {JAMA},
	author = {Ioannidis, John P. A.},
	month = mar,
	year = {2017},
	pmid = {28192565},
	keywords = {Animals, Biomedical Research, Data Interpretation, Statistical, Humans, Peer Review, Research, Publication Bias, Quality Improvement, Reproducibility of Results, Research Design, Selection Bias},
	pages = {1019--1020}
}

@article{manrai_methods_2016,
	title = {{METHODS} {TO} {ENHANCE} {THE} {REPRODUCIBILITY} {OF} {PRECISION} {MEDICINE}},
	volume = {21},
	issn = {2335-6936},
	language = {eng},
	journal = {Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing},
	author = {Manrai, Arjun K. and Patel, Chirag J. and Gehlenborg, Nils and Tatonetti, Nicholas P. and Ioannidis, John P. A. and Kohane, Isaac S.},
	year = {2016},
	pmid = {28004011},
	pmcid = {PMC5167531},
	pages = {180--182}
}

@article{ioannidis_failure_2015,
	title = {Failure to {Replicate}: {Sound} the {Alarm}},
	volume = {2015},
	issn = {1524-6205},
	shorttitle = {Failure to {Replicate}},
	abstract = {Science has always relied on reproducibility to build confidence in experimental results. Now, the most comprehensive investigation ever done about the rate and predictors of reproducibility in social and cognitive sciences has found that regardless of the analytic method or criteria used, fewer than half of the original findings were successfully replicated. While a failure to reproduce does not necessarily mean the original report was incorrect, the results suggest that more rigorous methods are long overdue.},
	language = {eng},
	journal = {Cerebrum: The Dana Forum on Brain Science},
	author = {Ioannidis, John P. A.},
	month = dec,
	year = {2015},
	pmid = {27420921},
	pmcid = {PMC4938249}
}

@article{ioannidis_why_2016,
	title = {Why {Most} {Clinical} {Research} {Is} {Not} {Useful}},
	volume = {13},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.1002049},
	abstract = {John Ioannidis argues that problem base, context placement, information gain, pragmatism, patient centeredness, value for money, feasibility, and transparency define useful clinical research. He suggests most clinical research is not useful and reform is overdue.},
	language = {eng},
	number = {6},
	journal = {PLoS medicine},
	author = {Ioannidis, John P. A.},
	month = jun,
	year = {2016},
	pmid = {27328301},
	pmcid = {PMC4915619},
	pages = {e1002049}
}

@article{iqbal_reproducible_2016,
	title = {Reproducible {Research} {Practices} and {Transparency} across the {Biomedical} {Literature}},
	volume = {14},
	issn = {1545-7885},
	doi = {10.1371/journal.pbio.1002333},
	abstract = {There is a growing movement to encourage reproducibility and transparency practices in the scientific community, including public access to raw data and protocols, the conduct of replication studies, systematic integration of evidence in systematic reviews, and the documentation of funding and potential conflicts of interest. In this survey, we assessed the current status of reproducibility and transparency addressing these indicators in a random sample of 441 biomedical journal articles published in 2000-2014. Only one study provided a full protocol and none made all raw data directly available. Replication studies were rare (n = 4), and only 16 studies had their data included in a subsequent systematic review or meta-analysis. The majority of studies did not mention anything about funding or conflicts of interest. The percentage of articles with no statement of conflict decreased substantially between 2000 and 2014 (94.4\% in 2000 to 34.6\% in 2014); the percentage of articles reporting statements of conflicts (0\% in 2000, 15.4\% in 2014) or no conflicts (5.6\% in 2000, 50.0\% in 2014) increased. Articles published in journals in the clinical medicine category versus other fields were almost twice as likely to not include any information on funding and to have private funding. This study provides baseline data to compare future progress in improving these indicators in the scientific literature.},
	language = {eng},
	number = {1},
	journal = {PLoS biology},
	author = {Iqbal, Shareen A. and Wallach, Joshua D. and Khoury, Muin J. and Schully, Sheri D. and Ioannidis, John P. A.},
	month = jan,
	year = {2016},
	pmid = {26726926},
	pmcid = {PMC4699702},
	keywords = {Biomedical Research, Conflict of Interest, Periodicals as Topic, Reproducibility of Results},
	pages = {e1002333}
}

@article{begley_reproducibility_2015,
	title = {Reproducibility in science: improving the standard for basic and preclinical research},
	volume = {116},
	issn = {1524-4571},
	shorttitle = {Reproducibility in science},
	doi = {10.1161/CIRCRESAHA.114.303819},
	abstract = {Medical and scientific advances are predicated on new knowledge that is robust and reliable and that serves as a solid foundation on which further advances can be built. In biomedical research, we are in the midst of a revolution with the generation of new data and scientific publications at a previously unprecedented rate. However, unfortunately, there is compelling evidence that the majority of these discoveries will not stand the test of time. To a large extent, this reproducibility crisis in basic and preclinical research may be as a result of failure to adhere to good scientific practice and the desperation to publish or perish. This is a multifaceted, multistakeholder problem. No single party is solely responsible, and no single solution will suffice. Here we review the reproducibility problems in basic and preclinical biomedical research, highlight some of the complexities, and discuss potential solutions that may help improve research quality and reproducibility.},
	language = {eng},
	number = {1},
	journal = {Circulation Research},
	author = {Begley, C. Glenn and Ioannidis, John P. A.},
	month = jan,
	year = {2015},
	pmid = {25552691},
	keywords = {Animals, Biomedical Research, Drug Evaluation, Preclinical, funding, Humans, journals, Reproducibility of Results, research integrity, universities},
	pages = {116--126}
}

@article{ioannidis_how_2014,
	title = {How to make more published research true},
	volume = {11},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.1001747},
	abstract = {In a 2005 paper that has been accessed more than a million times, John Ioannidis explained why most published research findings were false. Here he revisits the topic, this time to address how to improve matters. Please see later in the article for the Editors' Summary.},
	language = {eng},
	number = {10},
	journal = {PLoS medicine},
	author = {Ioannidis, John P. A.},
	month = oct,
	year = {2014},
	pmid = {25334033},
	pmcid = {PMC4204808},
	keywords = {Publications, Publishing, Research},
	pages = {e1001747}
}

@article{gilmore_progress_2017,
	title = {Progress toward openness, transparency, and reproducibility in cognitive neuroscience},
	issn = {1749-6632},
	doi = {10.1111/nyas.13325},
	abstract = {Accumulating evidence suggests that many findings in psychological science and cognitive neuroscience may prove difficult to reproduce; statistical power in brain imaging studies is low and has not improved recently; software errors in analysis tools are common and can go undetected for many years; and, a few large-scale studies notwithstanding, open sharing of data, code, and materials remain the rare exception. At the same time, there is a renewed focus on reproducibility, transparency, and openness as essential core values in cognitive neuroscience. The emergence and rapid growth of data archives, meta-analytic tools, software pipelines, and research groups devoted to improved methodology reflect this new sensibility. We review evidence that the field has begun to embrace new open research practices and illustrate how these can begin to address problems of reproducibility, statistical power, and transparency in ways that will ultimately accelerate discovery.},
	language = {eng},
	journal = {Annals of the New York Academy of Sciences},
	author = {Gilmore, Rick O. and Diaz, Michele T. and Wyble, Brad A. and Yarkoni, Tal},
	month = may,
	year = {2017},
	pmid = {28464561},
	keywords = {data sharing, open science, reproducibility}
}

@article{vasilevsky_reproducible_2017,
	title = {Reproducible and reusable research: are journal data sharing policies meeting the mark?},
	volume = {5},
	shorttitle = {},
	doi = {10.7717/peerj.3208},
	abstract = {BACKGROUND: There is wide agreement in the biomedical research community that research data sharing is a primary ingredient for ensuring that science is more transparent and reproducible. Publishers could play an important role in facilitating and enforcing data sharing; however, many journals have not yet implemented data sharing policies and the requirements vary widely across journals. This study set out to analyze the pervasiveness and quality of data sharing policies in the biomedical literature.
METHODS: The online author's instructions and editorial policies for 318 biomedical journals were manually reviewed to analyze the journal's data sharing requirements and characteristics. The data sharing policies were ranked using a rubric to determine if data sharing was required, recommended, required only for omics data, or not addressed at all. The data sharing method and licensing recommendations were examined, as well any mention of reproducibility or similar concepts. The data was analyzed for patterns relating to publishing volume, Journal Impact Factor, and the publishing model (open access or subscription) of each journal.
RESULTS: A total of 11.9\% of journals analyzed explicitly stated that data sharing was required as a condition of publication. A total of 9.1\% of journals required data sharing, but did not state that it would affect publication decisions. 23.3\% of journals had a statement encouraging authors to share their data but did not require it. A total of 9.1\% of journals mentioned data sharing indirectly, and only 14.8\% addressed protein, proteomic, and/or genomic data sharing. There was no mention of data sharing in 31.8\% of journals. Impact factors were significantly higher for journals with the strongest data sharing policies compared to all other data sharing criteria. Open access journals were not more likely to require data sharing than subscription journals.
DISCUSSION: Our study confirmed earlier investigations which observed that only a minority of biomedical journals require data sharing, and a significant association between higher Impact Factors and journals with a data sharing requirement. Moreover, while 65.7\% of the journals in our study that required data sharing addressed the concept of reproducibility, as with earlier investigations, we found that most data sharing policies did not provide specific guidance on the practices that ensure data is maximally available and reusable.},
	language = {eng},
	journal = {PeerJ},
	author = {Vasilevsky, Nicole A. and Minnier, Jessica and Haendel, Melissa A. and Champieux, Robin E.},
	year = {2017},
	pmid = {28462024},
	pmcid = {PMC5407277},
	keywords = {Data management, data sharing, Open data, reproducibility, Scholarly communication, Scientific communication},
	pages = {e3208}
}

@article{agostinelli_sox-11_2017,
	title = {{SOX}-11 detection in decalcified bone marrow tissue in mantle cell lymphoma patients, methodological issue on reproducibility and validity-reply},
	issn = {1532-8392},
	doi = {10.1016/j.humpath.2017.02.029},
	language = {eng},
	journal = {Human Pathology},
	author = {Agostinelli, Claudio and Sabattini, Elena and Pileri, Stefano A. and Fabbri, Marco},
	month = apr,
	year = {2017},
	pmid = {28461033}
}

@article{sabour_sox-11_2017,
	title = {{SOX}-11 detection in decalcified bone marrow tissue in mantle cell lymphoma patients, methodological issue on reproducibility and validity},
	issn = {1532-8392},
	doi = {10.1016/j.humpath.2017.02.027},
	language = {eng},
	journal = {Human Pathology},
	author = {Sabour, Siamak},
	month = apr,
	year = {2017},
	pmid = {28457728}
}

@article{soukup_prognostic_2017,
	title = {Prognostic {Performance} and {Reproducibility} of the 1973 and 2004/2016 {World} {Health} {Organization} {Grading} {Classification} {Systems} in {Non}-muscle-invasive {Bladder} {Cancer}: {A} {European} {Association} of {Urology} {Non}-muscle {Invasive} {Bladder} {Cancer} {Guidelines} {Panel} {Systematic} {Review}},
	issn = {1873-7560},
	shorttitle = {Prognostic {Performance} and {Reproducibility} of the 1973 and 2004/2016 {World} {Health} {Organization} {Grading} {Classification} {Systems} in {Non}-muscle-invasive {Bladder} {Cancer}},
	doi = {10.1016/j.eururo.2017.04.015},
	abstract = {CONTEXT: Tumour grade is an important prognostic indicator in non-muscle-invasive bladder cancer (NMIBC). Histopathological classifications are limited by interobserver variability (reproducibility), which may have prognostic implications. European Association of Urology NMIBC guidelines suggest concurrent use of both 1973 and 2004/2016 World Health Organization (WHO) classifications.
OBJECTIVE: To compare the prognostic performance and reproducibility of the 1973 and 2004/2016 WHO grading systems for NMIBC.
EVIDENCE ACQUISITION: A systematic literature search was undertaken incorporating Medline, Embase, and the Cochrane Library. Studies were critically appraised for risk of bias (QUIPS). For prognosis, the primary outcome was progression to muscle-invasive or metastatic disease. Secondary outcomes were disease recurrence, and overall and cancer-specific survival. For reproducibility, the primary outcome was interobserver variability between pathologists. Secondary outcome was intraobserver variability (repeatability) by the same pathologist.
EVIDENCE SYNTHESIS: Of 3593 articles identified, 20 were included in the prognostic review; three were eligible for the reproducibility review. Increasing tumour grade in both classifications was associated with higher disease progression and recurrence rates. Progression rates in grade 1 patients were similar to those in low-grade patients; progression rates in grade 3 patients were higher than those in high-grade patients. Survival data were limited. Reproducibility of the 2004/2016 system was marginally better than that of the 1973 system. Two studies on repeatability showed conflicting results. Most studies had a moderate to high risk of bias.
CONCLUSIONS: Current grading classifications in NMIBC are suboptimal. The 1973 system identifies more aggressive tumours. Intra- and interobserver variability was slightly less in the 2004/2016 classification. We could not confirm that the 2004/2016 classification outperforms the 1973 classification in prediction of recurrence and progression.
PATIENT SUMMARY: This article summarises the utility of two different grading systems for non-muscle-invasive bladder cancer. Both systems predict progression and recurrence, although pathologists vary in their reporting; suggestions for further improvements are made.},
	language = {eng},
	journal = {European Urology},
	author = {Soukup, Viktor and Čapoun, Otakar and Cohen, Daniel and Hernández, Virginia and Babjuk, Marek and Burger, Max and Compérat, Eva and Gontero, Paolo and Lam, Thomas and MacLennan, Steven and Mostafid, A. Hugh and Palou, Joan and van Rhijn, Bas W. G. and Rouprêt, Morgan and Shariat, Shahrokh F. and Sylvester, Richard and Yuan, Yuhong and Zigeuner, Richard},
	month = apr,
	year = {2017},
	pmid = {28457661},
	keywords = {1973 World Health Organization classification, 2004/2016 World Health Organization classification, Grade, Non–muscle-invasive bladder cancer, Prognosis, Progression, Recurrence, Repeatability, reproducibility}
}

@article{giannotti_carotid_2017,
	title = {Carotid atherosclerotic plaques standardised uptake values: software challenges and reproducibility},
	volume = {7},
	shorttitle = {Carotid atherosclerotic plaques standardised uptake values},
	doi = {10.1186/s13550-017-0285-0},
	abstract = {BACKGROUND: Positron emission tomography-computed tomography (PET-CT) carotid standardised uptake values (SUV) of (18)F-fluorodeoxyglucose ((18)FDG) have been proposed as an inflammatory biomarker for determining cerebrovascular diseases such as stroke. Consideration of varying methodological approaches and software packages is critical to the calculation of accurate SUVs in cross-sectional and longitudinal patient studies. The aim of this study was to investigate whether or not carotid atherosclerotic plaque SUVs are consistent and reproducible between software packages. (18)FDG-PET SUVs of carotids were taken in 101 patients using two different software packages. Quality assurance checks were performed to standardise techniques before commencing the analysis where data from five to seven anatomical sites were measured. A total of ten regions of interest were drawn on each site analysed. Statistical analyses were then performed to compare SUV measurements from the two software packages and to explore reproducibility of measurements. Lastly, the time taken to complete each analysis was measured and compared.
RESULTS: Statistically significant differences in SUV measurements, between the two software packages, ranging from 9 to 21.8\% were found depending on ROI location. In 79\% (n = 23) of the ROI locations, the differences between the SUV measurements from each software package were found to be statistically significant. The time taken to perform the analyses and export data from the software packages also varied considerably.
CONCLUSIONS: This study highlights the importance of standardising all aspects of methodological approaches to ensure accuracy and reproducibility. Physicians must be aware that when a PET-CT data set is analysed, subsequent follow-ups must be verified, if possible, with the same software package or cross-calibration between packages should be performed.},
	language = {eng},
	number = {1},
	journal = {EJNMMI research},
	author = {Giannotti, Nicola and O'Connell, Martin J. and Foley, Shane J. and Kelly, Peter J. and McNulty, Jonathan P.},
	month = dec,
	year = {2017},
	pmid = {28455733},
	keywords = {18F-FDG, Analysis, Carotid atherosclerotic plaque, Positron emission tomography–computed tomography},
	pages = {39}
}

@article{kafkafi_addressing_2017,
	title = {Addressing reproducibility in single-laboratory phenotyping experiments},
	volume = {14},
	issn = {1548-7105},
	doi = {10.1038/nmeth.4259},
	language = {eng},
	number = {5},
	journal = {Nature Methods},
	author = {Kafkafi, Neri and Golani, Ilan and Jaljuli, Iman and Morgan, Hugh and Sarig, Tal and Würbel, Hanno and Yaacoby, Shay and Benjamini, Yoav},
	month = apr,
	year = {2017},
	pmid = {28448068},
	pages = {462--464}
}

@article{heroux_questionable_2017,
	title = {Questionable science and reproducibility in electrical brain stimulation research},
	volume = {12},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0175635},
	abstract = {Electrical brain stimulation (EBS) is a trendy new technique used to change brain function and treat neurological, psychiatric and psychological disorders. We were curious whether the published literature, which is dominated by positive results, reflects the experience of researchers using EBS. Specifically, we wanted to know whether researchers are able to reproduce published EBS effects and whether they engage in, but fail to report, questionable research practices. We invited 976 researchers to complete an online survey. We also audited 100 randomly-selected published EBS papers. A total of 154 researchers completed the survey. Survey respondents had a median of 3 [1 to 6, IQR] published EBS papers (1180 total) and 2 [1 to 3] unpublished ones (380 total). With anodal and cathodal EBS, the two most widely used techniques, 45-50\% of researchers reported being able to routinely reproduce published results. When asked about how study sample size was determined, 69\% of respondents reported using the sample size of published studies, while 61\% had used power calculations, and 32\% had based their decision on pilot data. In contrast, our audit found only 6 papers where power calculations were used and a single paper in which pilot data was used. When asked about questionable research practices, survey respondents were aware of other researchers who selectively reported study outcomes (41\%) and experimental conditions (36\%), adjusted statistical analysis to optimise results (43\%), and engaged in other shady practices (20\%). Fewer respondents admitted to engaging in these practices themselves, although 25\% admitted to adjusting statistical analysis to optimize results. There was strong agreement that such practices should be reported in research papers; however, our audit found only two such admissions. The present survey confirms that questionable research practices and poor reproducibility are present in EBS studies. The belief that EBS is effective needs to be replaced by a more rigorous approach so that reproducible brain stimulation methods can be devised and applied.},
	language = {eng},
	number = {4},
	journal = {PloS One},
	author = {Héroux, Martin E. and Loo, Colleen K. and Taylor, Janet L. and Gandevia, Simon C.},
	year = {2017},
	pmid = {28445482},
	pages = {e0175635}
}

@article{zeiss_reproducibility_2017,
	title = {From {Reproducibility} to {Translation} in {Neurodegenerative} {Disease}},
	issn = {1930-6180},
	doi = {10.1093/ilar/ilx006},
	abstract = {Despite tremendous investment and preclinical success in neurodegenerative disease, effective disease-altering treatments for patients have remained elusive. One highly cited reason for this discrepancy is flawed animal study design and reporting. If this can be broadly remedied, reproducibility of preclinical studies will improve. However, without concurrent efforts to improve generalizability, these improvements may not translate effectively from animal experiments to more complex human neurodegenerative diseases. Mechanistic and phenotypic variability of neurodegenerative disease is such that most models are only able to interrogate individual aspects of complex phenomena. One approach is to consider animals as models of individual targets rather than as models of individual diseases and to migrate the concept of predictive validity from the individual model to the body of experiments that demonstrate translatability of a target. Both exploratory and therapeutic preclinical studies are dependent upon study design methods that promote rigor and reproducibility. However, the body of evidence that is needed to demonstrate efficacy in therapeutic studies is substantially broader than that needed for exploratory studies. In addition to requiring rigor within individual experiments, convincing evidence for therapeutic potential must assess the relationships between model choice, intended goal of the intervention, pharmacologic criteria, and integration of biomarker data with outcome measures that are clinically relevant to humans. It is conceivable that proof-of-concept studies will migrate to cell-based systems and that animal systems will be increasingly reserved for more distal translational purposes. If this occurs, it is likely to prompt reexamination of what the term "translational" truly means.},
	language = {eng},
	journal = {ILAR journal},
	author = {Zeiss, Caroline J.},
	month = apr,
	year = {2017},
	pmid = {28444192},
	keywords = {neurodegeneration, reproducibility, rigor, translation},
	pages = {1--9}
}

@article{restrepo-escobar_systematic_2017,
	title = {Systematic review of the literature on reproducibility of the interpretation of renal biopsy in lupus nephritis},
	issn = {1477-0962},
	doi = {10.1177/0961203317706556},
	abstract = {Objective Before using a test, it should be determined whether the results are reliable. The reliability of the interpretation of renal biopsy in patients with lupus nephritis has not been clearly elucidated. Our objective was to estimate inter and intra-observer reliability of the histological classification, as well as activity and chronicity indices in renal biopsy of patients with lupus nephritis. Methods We conducted a systematic search of the literature, which included articles in any language, using PubMed, Embase, Cochrane and Lilacs databases. Search terms included were: reproducibility, reliability, agreement, systemic lupus erythematosus and lupus nephritis. Comparative studies with any design were included, regardless of the year or the language of publication. Two investigators, independently, screened the literature published in accordance with pre-established inclusion and exclusion criteria. Results We found 13 relevant studies. Inter-observer reproducibility of most measurements was moderate or low, despite the fact that, in most cases, the readings were made by expert nephropathologists. There was great diversity among designs, participants, including samples and outcomes evaluated in different studies. Although there are too many reports on the clinical use, studies evaluating the reliability of classifications on renal biopsy in lupus nephritis are rare. The quality of the methodological design and reporting was fair. Conclusion The interpretation of renal biopsy in lupus nephritis is poorly reproducible, causing serious doubts about its validity and its clinical application. As it can lead to serious diagnosis, treatment and prognosis errors, it is necessary to intensify research in this field.},
	language = {eng},
	journal = {Lupus},
	author = {Restrepo-Escobar, M. and Granda-Carvajal, P. A. and Jaimes, F.},
	month = jan,
	year = {2017},
	pmid = {28441914},
	keywords = {Biopsy, nephritis, reliability, renal lupus, Reproducibility of Results, systematic review},
	pages = {961203317706556}
}

@article{moody_rigor_2017,
	title = {Rigor, {Reproducibility} and in vitro {CSF} assays: {The} {Devil} in the {Details}},
	issn = {1531-8249},
	shorttitle = {Rigor, {Reproducibility} and in vitro {CSF} assays},
	doi = {10.1002/ana.24940},
	language = {eng},
	journal = {Annals of Neurology},
	author = {Moody, Olivia A. and Talwar, Sahil and Jenkins, Meagan A. and Freeman, Amanda A. and Trotti, Lynn Marie and García, Paul S. and Bliwise, Donald and Lynch, Joseph W. and Cherson, Brad and Hernandez, Eric M. and Feldman, Neil and Saini, Prabhjyot and Rye, David B. and Jenkins, Andrew},
	month = apr,
	year = {2017},
	pmid = {28440033}
}

@article{fontoura-andrade_improving_2017,
	title = {Improving reproducibility and external validity. {The} role of standardization and data reporting of laboratory rat husbandry and housing},
	volume = {32},
	issn = {1678-2674},
	doi = {10.1590/S0102-865020170030000010},
	abstract = {Purpose:: To identify the most relevant flaws in standardization in husbandry practices and lack of transparency to report them. This review proposes some measures in order to improve transparency, reproducibility and eventually external validity in experimental surgery experiments with rat model.
Methods:: We performed a search of scientific articles in PUBMED data base. The survey was conducted from august 2016 to January 2017. The keywords used were "reproducibility", "external validity", "rat model", "rat husbandry", "rat housing", and the time frame was up to January 2017. Articles discarded were the ones which the abstract or the key words did not imply that the authors would discuss any relationship of husbandry and housing with the reproducibility and transparency of reporting animal experiment. Reviews and papers that discussed specifically reproducibility and data reporting transparency were laboriously explored, including references for other articles that could fulfil the inclusion criteria. A total of 246 articles were initially found but only 44 were selected.
Results:: Lack of transparency is the rule and not the exception when reporting results with rat model. This results in poor reproducibility and low external validity with the consequence of considerable loss of time and financial resources. There are still much to be done to improve compliance and adherence of researchers, editors and reviewers to adopt guidelines to mitigate some of the challenges that can impair reproducibility and external validity.
Conclusions:: Authors and reviewers should avoid pitfalls of absent, insufficient or inaccurate description of relevant information the rat model used. This information should be correctly published or reported on another source easily available for readers. Environmental conditions are well known by laboratory animal personnel and are well controlled in housing facilities, but usually neglected in experimental laboratories when the rat model is a novelty for the researcher.},
	language = {eng},
	number = {3},
	journal = {Acta Cirurgica Brasileira},
	author = {Fontoura-Andrade, José Luiz and Amorim, Rivadávio Fernandes Batista de and Sousa, João Batista de},
	month = mar,
	year = {2017},
	pmid = {28403350},
	pages = {251--262}
}

@inproceedings{lmucs-papers:Leisch:2002,
  author = {Friedrich Leisch},
  title = {Sweave: Dynamic Generation of Statistical Reports Using
		  Literate Data Analysis},
  booktitle = {Compstat 2002 --- Proceedings in Computational
		  Statistics},
  pages = {575--580},
  year = 2002,
  editor = {Wolfgang H{\"a}rdle and Bernd R{\"o}nz},
  publisher = {Physica Verlag, Heidelberg},
  note = {ISBN 3-7908-1517-9},
  url = {http://www.stat.uni-muenchen.de/~leisch/Sweave}
}


@book{knuth_literate_1992,
	address = {Stanford, Calif.},
	edition = {1 edition},
	title = {Literate {Programming}},
	isbn = {978-0-937073-80-3},
	abstract = {This anthology of essays from Donald Knuth, "the father of computer science," and the inventor of literate programming includes early essays on related topics such as structured programming, as well as The Computer Journal article that launched literate programming itself. Many examples are given, including excerpts from the programs for TeX and METAFONT. The final essay is an example of CWEB, a system for literate programming in C and related languages.This volume is first in a series of Knuth's collected works.},
	language = {English},
	publisher = {Center for the Study of Language and Inf},
	author = {Knuth, Donald E.},
	month = jun,
	year = {1992}
}


@misc{gardner_text_2017,
	type = {webpage},
	title = {Text - {S}.3084 - 114th {Congress} (2015-2016): {American} {Innovation} and {Competitiveness} {Act}},
	copyright = {Text is government work},
	shorttitle = {Text - {S}.3084 - 114th {Congress} (2015-2016)},
	url = {https://www.congress.gov/bill/114th-congress/senate-bill/3084/text/enr},
	abstract = {Text for S.3084 - 114th Congress (2015-2016): American Innovation and Competitiveness Act},
	language = {eng},
	urldate = {2020-02-29},
	author = {Gardner, Cory},
	month = jan,
	year = {2017},
	keywords = {Reproducible research, Replicability},
	file = {Snapshot:/Users/nathanmalmberg/Zotero/storage/JYENN7KP/enr.html:text/html}
}

@book{national_academies_of_sciences_reproducibility_2019,
	title = {Reproducibility and {Replicability} in {Science}},
	isbn = {978-0-309-48616-3},
	url = {https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science},
	abstract = {Download a PDF of "Reproducibility and Replicability in Science" by the National Academies of Sciences, Engineering, and Medicine for free.},
	language = {en},
	urldate = {2020-02-29},
	author = {National Academies of Sciences, Engineering},
	month = may,
	year = {2019},
	doi = {10.17226/25303},
	file = {Snapshot:/Users/nathanmalmberg/Zotero/storage/P6KM8FEW/reproducibility-and-replicability-in-science.html:text/html}
}